# Dockerfile for n8n with ffmpeg, yt-dlp, and whisper.cpp (SMTE vPPLX-OS - Multi-Stage Build)

# ---- Stage 1: Builder ----
# This stage will download sources, compile whisper.cpp, and download models.
FROM alpine:3.19 AS builder

# Install build-time dependencies for whisper.cpp, git for cloning,
# bash for scripts, curl for model download (safer than relying on whisper.cpp's script internals)
RUN apk update && \
    apk add --no-cache \
    git \
    build-base \
    cmake \
    bash \
    curl

# Clone whisper.cpp, initialize submodules, and compile
WORKDIR /app
RUN echo "Cloning whisper.cpp (main repository)..." && \
    git clone https://github.com/ggerganov/whisper.cpp.git . && \
    echo "Initializing and updating Git submodules (this will fetch ggml and its contents)..." && \
    git submodule init && \
    git submodule update --init --recursive && \
    echo "Verifying crucial submodule file ggml/src/ggml.c:" && \
    ls -lh ggml/src/ggml.c && \
    echo "Attempting to build whisper.cpp (default 'make' target)..." && \
    make # Builds 'main' and other executables, typically in ./ or ./bin/

# Download the model using whisper.cpp's script
# The executable 'main' built by 'make' is in the current directory (/app)
# The models script will place models in /app/models/
RUN bash ./models/download-ggml-model.sh small


# ---- Stage 2: Final Runtime Image ----
# Start from the official n8n Alpine image
FROM n8nio/n8n:1.94.0

# Switch to root for installations
USER root

# Install runtime dependencies:
# - ffmpeg: for media processing
# - python3, py3-pip: for yt-dlp
# - bash: as some scripts might expect it (including potentially whisper.cpp's model script if run differently)
# - curl: (Optional, but often useful. If model download script needed it explicitly)
# yt-dlp will be installed via pip.
RUN apk update && \
    apk add --no-cache \
    ffmpeg \
    python3 \
    py3-pip \
    bash && \
    # curl is useful if you need to download things at runtime, not strictly for this setup if models are copied
    # apk add --no-cache curl && \
    pip3 install --no-cache-dir --break-system-packages yt-dlp && \
    rm -rf /var/cache/apk/*

# Create the target directory for whisper.cpp artifacts
WORKDIR /opt/whisper.cpp

# Copy only the necessary compiled whisper.cpp executables and the downloaded models
# from the builder stage.
# The 'main' executable from whisper.cpp's Makefile is created in its root directory.
# Other tools might be in ./bin if generated by CMake via make.
# We will copy the entire whisper.cpp directory from builder for simplicity,
# as it contains main, the models subdir, and any other necessary runtime files.
COPY --from=builder /app /opt/whisper.cpp

# Set the PATH to include the whisper.cpp directory where 'main' is.
ENV PATH="/opt/whisper.cpp:${PATH}"

# Switch back to the non-root n8n user
USER node

# Set the working directory for n8n (optional, n8n base image might do this)
WORKDIR /home/node

# Base n8n image handles the application start (CMD ["n8n"])
# The n8n user data will be in /home/node/.n8n which should map to your persistent disk at /opt/render/.n8n
# Ensure N8N_USER_FOLDER=/home/node/.n8n is NOT set if you want Render's default /opt/render/.n8n mapping via disk to work seamlessly
# Or, if N8N_USER_FOLDER=/opt/render/.n8n is set, that's fine. Given your env vars previously, /opt/render/.n8n is what n8n will use.

# Expose n8n port (already handled by base image and Render settings)
# EXPOSE 5678
